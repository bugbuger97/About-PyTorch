{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1의 목표\n",
        "  - 경사하강법\n",
        "  - 다중 선형 회귀\n",
        "\n",
        "# 경사하강법\n",
        "- gradient descent(= steepest descent)\n",
        "- first-order iterative optimization algorithm for finding a local minimum of a differentiable function.\n",
        "- 미분 가능한 함수의 최소값을 근사하기 위한 알고리즘의 일종\n",
        "- 입력값(x)을 조금씩 변경해가며 함수값을 최소로 만드는(y가 최소가 되는) 값을 찾아가는데, 이때 x값을 증가시킬지 감소시킬지를 미분값을 통해 파악한다.\n",
        "- 학습의 목적: 가능한 한 최적의 W를 찾는 것이며, 실제로는 많은 수의 가중치 및 바이어스에 대해 최적화를 해야하며 손실 함수도 복잡한 형태인 경우가 많으므로 그래프를 정확히 알 수 없음.\n",
        "- 현재의 모델 변수 값(W, b)에 대한 손실 함수 값을 근거로 삼아 조금씩 손실 함수 값을 감소시키는 방향으로 학습이 진행됨.\n",
        "- 문제) cost(W)를 감소시키기 위해서 어디로 가야하는지 어떻게 알 수 있나?\n",
        "  - cost(W)의 미분값이 양수이면, W를 감소시킨다.\n",
        "  - cost(W)의 미분값이 음수이면, W를 증가시킨다.\n",
        "  - 최소값 및 극소값인 경우, 미분값이 0이 됨.\n",
        "  - 경사하강법의 업데이트 수식\n",
        "    - W = W - a*∂cost(W)/∂W\n",
        "- 극값에 가까워질수록 점점 조금씩 움직여야 제대로 도달할 수 있음.\n",
        "  - 같은 학습률을 쓰더라도 극값에 다가갈수록 미분값 자체의 절대값이 작아지므로 자연스럽게 해결됨.\n",
        "- 학습률이 너무 높을 경우, 수렴하지 못하고 발산함.\n",
        "- 학습률이 너무 낮을 경우, 최소값이 아닌 국소 최소값(local minimum)에 빠지게 되어 더 좋은 성능을 낼 수 있는 기회를 살리지 못함.\n",
        "\n",
        "# 선형회귀\n",
        "- 일반적인 머신러닝/딥러닝 학습 과정\n",
        "  - 1. 데이터 셋 생성 혹은 로드\n",
        "    - x_train = torch.FloatTensor([1,2,3]).view(-1,1)\n",
        "    - y_train = torch.FloatTensor([4,8,12]).view(-1,1)\n",
        "\n",
        "  - 2. 모델 생성 및 초기화\n",
        "    - W = torch.zeros(1, requires_grad=True)\n",
        "    - b = torch.zeros(1, requires_grad=True)\n",
        "    - opimizer = optim.SGD([W,b], lr=0.01) # optimizer 설정\n",
        "    - nb_epochs = 1000 # 원하는 만큼 경사 하강법을 반복\n",
        "\n",
        "  - 3. 학습 루프\n",
        "    - 1. 예측값 도출\n",
        "      - for epoch in range(nb_epochs + 1):\n",
        "          - hypo = x_train * W + b # 예측값 도출, 가설 = 예측값\n",
        "    - 2. 손실 함수값 계산\n",
        "          - cost = torch.mean((hypo - y_train)**2)\n",
        "    - 3. 미분값 계산\n",
        "          - optimizer.zero_grad() # 미분값 초기화 -> 초기화를 하지 않으면 미분값이 계속 중첩됨. 따라서 업데이트 후 필요없어진 미분값에 대해선 반드시 zero_grad()로 지워주어야 함.\n",
        "          - cost.backward() # 미분값 계산\n",
        "    - 4. 경사하강법 업데이트 수행\n",
        "          - optimizer.step()\n",
        "    - 5. 정해진 반복 횟수에 도달하거나 학습 종료 조건 만족시 break\n",
        "          - if epoch % 100 == 0:\n",
        "            - print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()))\n",
        "\n",
        "- torch.optim에는 최적화를 위한 다양한 클래스가 구현되어 있으며, PyTorch에서는 이와 같이 최적화를 담당하는 객체를 optimizer라고 함.\n",
        "- SGD는 Stochastic(추측 통계학) Gradient Descent의 줄임말로 가장 기본적인 optimizer이다.\n",
        "  - optimizer = optim.SGD([W,b], lr = 0.01)\n",
        "    - 최적화를 수행할 대상 변수들을 첫번째 매개변수로 입력하고, 학습률을 lr이라는 이름의 매개변수로 지정함.\n",
        "- 이 이후, .zero_grad(), .backward(), .step() 등의 메서드들을 호출하여 최적화를 수행하게 됨.\n",
        "  - zero_grad(): 미분값 초기화\n",
        "  - backward(): 현재 손실 함수값을 바탕으로 미분값 계산\n",
        "  - step(): 현재 미분값 및 학습률을 바탕으로 변수 업데이트 1회 수행\n",
        "\n",
        "# 다중 선형 회귀\n",
        "- 다중 선형회귀: 여러 개의 독립 변수에 대한 선형회귀\n",
        "- 선형 회귀에선 하나의 독립 변수 x에만 의해서 y가 결정되었지만, 다양한 독립 변수들에 의해서 y가 결정된다.\n",
        "- 독립변수들과 y의 관계를 선형으로 가정함.\n",
        "- y = W1*x1 + W2*x2 + W3*x3 + b\n",
        "- 다중 선형회귀에서 벡터 형태의 W 및 스칼라 형태의 b로 변수들을 생성한 후, 행렬 행태의 학습 데이터에 대해서 행렬 연산으로 간편하게 예측값을 계산해낼 수 있다.\n",
        "  - b: 브로드캐스팅 기능 활용\n",
        "  - 이와 같이 구현하지 않으면, W1 ~ W3 각각에 대해 변수를 생성하여야 하고 독립변수 숫자가 늘어나면 더욱 번거로워짐."
      ],
      "metadata": {
        "id": "RV20m6kPbuPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "0Ys6fDsuPgrN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋 생성 및 W, b 선언\n",
        "\n",
        "# 1. 데이터 셋 생성\n",
        "x_train = torch.FloatTensor([[73, 80,75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 80],\n",
        "                             [96, 98, 100]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196]])\n",
        "\n",
        "# 2. 모델 생성 및 초기화\n",
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# 3. optimizer 설정\n",
        "optimizer = torch.optim.SGD([W,b], lr=1e-5)\n",
        "nb_epochs = 1000"
      ],
      "metadata": {
        "id": "6EUt1Z8sKvKT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습 루프: 행렬 연산을 이용한 예측값 계산\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  # 가설 및 손실 함수\n",
        "  hypo = x_train.matmul(W) + b # 행렬 연산 필요성: 행렬 형태의 데이터 셋, 변수 생성 및 연산을 활용하지 않으면 각 독립 변수마다 그리고 각 가중치마다 변수를 따로 생성해야 하며 가설 수식도 더 복잡해짐.\n",
        "  cost = torch.mean((hypo - y_train)**2)\n",
        "  # 경사하강법\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  # 100번마다 로그 출력\n",
        "  w_list = W.view(-1).tolist()\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch:4d} / {nb_epochs} w1: {w_list[0]:.3f}, w2: {w_list[1]:.3f}, w3: {w_list[2]:.3f}, b: {b.item():.3f} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTQkpM2zP4ww",
        "outputId": "2efa5e34-9659-4d89-bb4e-708d854ece5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0 / 1000 w1: 0.316, w2: 0.320, w3: 0.313, b: 0.004 Cost: 32036.250000\n",
            "Epoch  100 / 1000 w1: 0.684, w2: 0.688, w3: 0.652, b: 0.008 Cost: 6.938233\n",
            "Epoch  200 / 1000 w1: 0.694, w2: 0.693, w3: 0.637, b: 0.008 Cost: 6.573693\n",
            "Epoch  300 / 1000 w1: 0.704, w2: 0.698, w3: 0.622, b: 0.008 Cost: 6.232265\n",
            "Epoch  400 / 1000 w1: 0.714, w2: 0.702, w3: 0.608, b: 0.008 Cost: 5.912263\n",
            "Epoch  500 / 1000 w1: 0.724, w2: 0.706, w3: 0.594, b: 0.008 Cost: 5.612178\n",
            "Epoch  600 / 1000 w1: 0.733, w2: 0.710, w3: 0.581, b: 0.008 Cost: 5.330629\n",
            "Epoch  700 / 1000 w1: 0.743, w2: 0.713, w3: 0.568, b: 0.008 Cost: 5.066350\n",
            "Epoch  800 / 1000 w1: 0.752, w2: 0.717, w3: 0.555, b: 0.008 Cost: 4.818101\n",
            "Epoch  900 / 1000 w1: 0.761, w2: 0.720, w3: 0.543, b: 0.008 Cost: 4.584816\n",
            "Epoch 1000 / 1000 w1: 0.769, w2: 0.722, w3: 0.532, b: 0.008 Cost: 4.365375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론\n",
        "test_data = torch.FloatTensor([[73, 66, 70]])\n",
        "prediction = test_data.matmul(W) + b\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQza_UmZRdRM",
        "outputId": "6331dbbd-814d-4c1e-d71d-1bd7c4314ffb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[141.0700]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 목표\n",
        "- torch.nn.Module을 이용하여 클래스 형태로 다중 선형 회귀 구현\n",
        "\n",
        "# torch.nn.Module\n",
        "  - PyTorch에서 모든 신경망 모델의 base class가 되는 클래스\n",
        "  - PyTorch에서 신경망 클래스를 구현할 때는 torch.nn.Module을 상속받아야 함.\n",
        "  - Module은 또 다른 Module을 attribute로 가질 수 있음.\n"
      ],
      "metadata": {
        "id": "4qs02Z3nbaHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.nn.Module을 이용한 다중 선형 회귀 모델\n",
        "# 신경망 구성을 위한 다양한 클래스가 포함되어 있는 torch.nn은 주로 nn이라는 별칭으로 import하는 경우가 많음.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "JNrw-aX8c6ZM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultipleLinearRegressionModel(nn.Module): # nn.Module을 상속받았다는 것을 알 수 있다.\n",
        "  def __init__(self): # 생성자 함수\n",
        "    super().__init__() # super(): base class의 메서드를 활용한다는 의미 -> 먼저 base class인 nn.Module의 생성자 함수를 호출한 다음 해당 모델에 필요한 가중치 및 편향 등을 생성해주어야 함.\n",
        "    self.linear = nn.Linear(3,1) # 3개의 독립 변수로부터 1개의 값을 예측한다는 의미(간단한 신경망), y = w1*x1 + w2*x2 + w3*x3 + b\n",
        "    # nn.Linear: 전통적인 fully connected neural network를 구성할 때 사용하는 클래스\n",
        "    # 기본적으로 nn.Linear(입력 크기, 출력 크기) 형태로 호출\n",
        "\n",
        "  # nn.Module에서 forward라는 메서드는 해당 모델에 입력이 들어왔을 때, 어떻게 계산해서 결과값을 만들고 반환할지를 명시하는 메서드임.\n",
        "  # 오버라이딩이기 때문에 원하는대로 매개변수 개수를 설정할 수 있으나, 현재는 텐서 하나만을 x라는 이름으로 받음.\n",
        "  def forward(self, x): # nn.Module 객체에서 변수명(입력값) 형태로 호출하면, forward 메서드가 호출됨.\n",
        "    return self.linear(x) # self.linear(x): __init__에서 생성된 self.linear객체의 forward 메서드를 호출한다."
      ],
      "metadata": {
        "id": "tj1DHYgJdaSW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.nn.functional은 인공지능 프로그래밍에 쓰이는 다양한 함수들을 포함하고 있으며 대표적인 손실 함수들이 있음.\n",
        "import torch.nn.functional as F\n",
        "# MSE의 경우에는 mse_loss라는 이름의 함수로 존재하며, 사용할 때는 mse_loss(예측값, 레이블) 형태로 호출하면 됨.\n",
        "# cost = F.mse_loss(prediction, y_train): 평균 제곱 오차(MSE)"
      ],
      "metadata": {
        "id": "YK_5n0D9d3W4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Module에서 parameters() 메서드를 호출하면, 내부의 모든 가중치 및 편향을 모아서 반환함.\n",
        "# W, b를 직접 선언할 때와는 달리 nn.Module을 이용한 클래스 형태로 모델을 구현하게 되면, optimizer 선언 시 parameters() 메서드를 활용해서 최적화를 진행할 변수들을 입력함.\n",
        "# PyTorch에서는 가중치 및 편향을 초기화하는 default 방식이 따로 존재하기 때문에 초기화된 상태로 출력해도 zero tensor가 아닌 것을 확인할 수 있음.\n",
        "model = MultipleLinearRegressionModel()\n",
        "print(list(model.parameters())) # W 3개와 b 1개 반환\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1HDKCotAdo8",
        "outputId": "a5943ebf-abce-4b03-afc1-f1abb4374ea1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.2276, -0.1599,  0.2143]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3560], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer in Pytorch: 모델의 loss 값을 줄이는 방향으로 parameters를 조절함.\n",
        "print(optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqvYujKxAw4g",
        "outputId": "76bb36d9-4936-4531-fb4d-9e61c2486f72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 코드\n",
        "class MultipleLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3,1)\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "qfHAzyg1ETbt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 코드\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 80],\n",
        "                             [96, 98, 100]])\n",
        "\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196]])"
      ],
      "metadata": {
        "id": "VCPgy9HfFw3-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultipleLinearRegressionModel()\n",
        "print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8N_6JRdGxve",
        "outputId": "bbcb4702-4585-44c4-813f-a8c2ad3595df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.1146, -0.2252, -0.2295]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4147], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "nb_epochs = 1000"
      ],
      "metadata": {
        "id": "ZyCGzivVH6FC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(nb_epochs + 1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch: {epoch} / {nb_epochs} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWl9j0SyIkhZ",
        "outputId": "d75ea1cb-11b4-456e-9c29-fbdb516ba5ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 / 1000 Cost: 83.152328\n",
            "Epoch: 100 / 1000 Cost: 4.003900\n",
            "Epoch: 200 / 1000 Cost: 3.755467\n",
            "Epoch: 300 / 1000 Cost: 3.524437\n",
            "Epoch: 400 / 1000 Cost: 3.309574\n",
            "Epoch: 500 / 1000 Cost: 3.109640\n",
            "Epoch: 600 / 1000 Cost: 2.923544\n",
            "Epoch: 700 / 1000 Cost: 2.750206\n",
            "Epoch: 800 / 1000 Cost: 2.588792\n",
            "Epoch: 900 / 1000 Cost: 2.438357\n",
            "Epoch: 1000 / 1000 Cost: 2.298064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론\n",
        "test_data = torch.FloatTensor([[73, 66, 70]])\n",
        "prediction = model(test_data)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCpYOt53Jwit",
        "outputId": "38c02cea-58a9-4834-f500-51843c5a61c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[142.0052]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}
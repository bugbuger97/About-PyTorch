{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning\n",
        "\n",
        "- 1. 인공지능, 머신러닝, 딥러닝 개요\n",
        "  - 인간의 지식 혹은 지능이 필요한 일을 수행할 수 있는 컴퓨터 프로그램 혹은 이와 관련된 이론 등의 분야\n",
        "  - 인공지능 연구를 거듭할수록 사람에게는 쉬운 일이 컴퓨터로 수행하기에는 무척 어려운 문제가 발생하였음.\n",
        "  - 컴퓨터는 0, 1로 이루어진 명령어들을 어마어마하게 빠른 속도로 계산하는 장치임.\n",
        "  - 사람이 일상적으로 하는 언어활동, 사물 인지, 걷고 뛰는 등의 운동과 같은 작업들은 프로그래머 입장에서 컴퓨터에게 그 규칙을 알려줄 수가 없음.\n",
        "  - 인공지능 발달 과정\n",
        "    - 1. 지식 베이스(Knowledge-base)기반 접근\n",
        "      - 단점: 제한된 분야에서는 효과적이지만, 지식 기반 접근 방식으로 인공지능을 완벽히 만들긴 어려움.\n",
        "    - 2. 기계학습(Machine learning)\n",
        "      - 데이터들로부터 학습하여 스스로 발전할 수 있는 컴퓨터 기술 및 그 분야\n",
        "      - Raw Data -> Feature Engineering -> Feature Vector\n",
        "      - 특징점(특징 벡터)를 뽑아주고 나면 스스로 학습\n",
        "    - 3. 표현 학습\n",
        "      - 데이터들로부터 특징점 추출을 자동으로 학습하는 기계학습의 일종\n",
        "      - 일반적인 표현 학습은 한 단계만에 원본 데이터를 특징점으로 변환함.\n",
        "      - 단점 한 단계만에 특징점을 추출하기 때문에 성능이 좋지 않음.\n",
        "    - 4. 딥러닝\n",
        "      - 표현 학습 및 인공 신경망에 기반하여 계층적으로 특징점 추출을 학습하는 기계학습의 일종.\n",
        "      - 표현학습의 단점을 보완하여 여러 단계를 거쳐서 특징점을 추출함.\n",
        "  - 인공신경망\n",
        "    - 실제 뇌의 신경세포 및 신경망 구조를 기반으로 이와 유사한 구조를 갖도록 만든 기계학습 방식의 일종\n",
        "  - Artifical Intelligence > Machine Learning > Representation > Deep Learning\n",
        "  - 디지털 이미지\n",
        "    - 픽셀이라는 최소 단위가 여러 개 모여서 이루어짐\n",
        "    - 흑백 이미지를 기준으로 픽셀은 보통 0~255의 값을 가지며, 값이 높을수록 밝음.\n",
        "    - 컬러 이미지는 Red, Green, Blue 채널이라고 불리는 흑백 이미지 3장으로 이루어짐.\n",
        "  - 규칙 기반 인공지능\n",
        "    - 입력 -> 규칙 기반 프로그램 -> 출력\n",
        "    - ex) 이미지 분류: 픽셀값 차이가 더 작은 쪽으로 분류\n",
        "  - 기계학습\n",
        "    - 입력 -> 규칙 기반 특징점 -> 특징점 및 결과 매핑 -> 출력\n",
        "    - Input -> Features -> Training/Inference -> Output\n",
        "  - 표현학습\n",
        "    - 입력 -> 학습 기반 특징점 -> 특징점 및 결과 매핑 -> 출력\n",
        "    - Input -> Training/Inference(Features -> Classification) -> Output\n",
        "  - 딥러닝\n",
        "    - 입력 -> 단순 특징점 -> 추상적 특징점들 -> 특징점 및 결과 매핑 -> 출력\n",
        "    - Input -> Training/Inference(Simple features -> Abstract features -> More abstract features -> Classification) -> Output\n",
        "  - Low-level feature vs High-level feature\n",
        "    - Low-level feature\n",
        "      - 컴퓨터 입장에서 익숙하고 수치로 나타내기 좋은 특징점\n",
        "      - 이미지를 픽셀 하나하나가 눈에 보일만큼 확대해서 분석한 특징점이라고 볼 수 있음.\n",
        "    - High-level feature\n",
        "      - 사람에게 익숙한 특징점이나 컴퓨터 입장에서 이해하거나 수치화하기 어려움.\n",
        "      - 이미지의 전체 모양, 질감 등에 해당함.\n",
        "    - 저레벨 특징점들을 분석해서 보다 고레벨의 특징점을 추출할 수 있음.\n",
        "  - 딥러닝의 부상 배경\n",
        "    - 훈련 자료의 양이 늘어남.\n",
        "    - 컴퓨팅 환경의 발달로 학습이 수월해짐.\n",
        "    - 학습 안정성 및 과적합 문제를 해결하는 각종 학습 기법의 발달\n",
        "    - 컴퓨터 비전\n",
        "      - 컴퓨터나 기계가 시각 정보를 분석하여 목표 업무를 수행하도록 하는 기술\n",
        "      - Classification, Localization, Object Detection, Segmentation\n",
        "      - Style transfer\n",
        "      - 얼굴 인식\n",
        "      - 문자 인식(Optical Character Recognition, OCR)\n",
        "      - 자연어 처리\n",
        "      - 감성 분석\n",
        "        - sentence matrix -> convolutional feature map -> pooled representation\n",
        "        -> softmax\n",
        "      - 기계 번역\n",
        "      - 문장 합성\n",
        "      - 음성 인식(Speech-to-text,STT): 음성 데이터를 텍스트 데이터로 변환하는 기술\n",
        "        - Input -> Stateful Model(Recurrent Neural Network) -> Model\n",
        "      - 음성 합성(Text-to-speech, TTS)\n",
        "        - 텍스트 데이터를 사람이 읽은 것 같은 음성 데이터로 변환하는 기술\n",
        "\n",
        "- 2. 머신러닝 학습 및 활용 과정\n",
        "  - 머신러닝 학습 과정\n",
        "    - 머신러닝/딥 러닝 모델은 일반적으로 학습 단계를 거쳐서 특정 작업을 똑똑하게 수행할 수 있음.\n",
        "    - 학습이 완료된 모델은 처음 보는 데이터에 대해서 예측값을 내놓게 되며, 이 과정을 예측 혹은 추론(inference)이라고 함.\n",
        "    - 학습 과정에서는 현재 모델이 얼마나 주어진 문제를 잘 풀었는지에 대해 피드백을 주고, 이를 바탕으로 모델의 성능이 점점 개선되는 방향으로 학습이 진행됨.\n",
        "    - 추론 과정에서는 주어진 입력에 대해 모델이 내놓은 답안에 대해 피드백이 없음.\n",
        "      - 모델의 예측값을 우리가 활용할 목적으로 추론을 수행함.\n",
        "\n",
        "- 3. 머신러닝 작업 종류\n",
        "  - 1. 지도 학습(Supervised Learning)\n",
        "    - 컴퓨터에게 각 데이터에서의 정답(레이블, label)이 무엇인지 알려주고 학습시키는 방법\n",
        "    - 분류\n",
        "      - 주어진 데이터가 어떤 클래스(class)에 속하는지를 예측하는 작업\n",
        "    - 회귀\n",
        "      - 주어진 입력 데이터로부터 원하는 결과값 자체를 예측하는 작업\n",
        "\n",
        "  - 2. 비지도 학습(Unsupervised Learning)\n",
        "    - 데이터만 주어지고 정답이 주어지지 않는 작업 유형\n",
        "    - 이때, 머신러닝 모델은 주로 어떤 데이터들끼리 하나의 클래스로 묶을 수 있을지를 학습함.\n",
        "    - 연관 규칙, 군집\n",
        "    - 레이블 없이 학습\n",
        "    - 데이터의 숨겨진 구조/특징 발견\n",
        "\n",
        "  - 3. 강화 학습(Reinforcement Learning)\n",
        "    - 데이터가 주어진는 것이 아니라, 머신러닝 모델이 직접 행동을 하며 각각의 행동에 대해 잘했는지 못했는지 피드백을 받으며 학습하는 방식\n",
        "    - 보상 시스템으로 학습\n",
        "    - 의사결정을 위한 최적의 액션 선택\n",
        "\n",
        "- 4. 선형 회귀(Linear Regression)\n",
        "  - 우리가 얻을 수 있는 독립 변수들(데이터)로부터 우리가 알고자 하는 결과값(레이블)을 도출해내는 모델을 얻는 머신러닝 기법\n",
        "  - 회귀에서의 레이블은 연속된 범위를 갖고 있다고 볼 수 있음.\n",
        "  - 데이터와 레이블의 관계를 선형으로 가정하고 문제를 푸는 방식\n",
        "  - 이미 데이터(x)와 레이블(y)을 다 알고 있는 데이터들은 모델 학습에 사용되므로 학습 데이터 셋이라고 하며, 데이터 밖에 없고 레이블을 예측하여야 하는 경우에는 테스트 데이터 셋이라고 함.\n",
        "  - 머신러닝에서 데이터와 레이블 사이의 관계에 대해 가정한 사항들을 가설(hypothesis)이라고 함.\n",
        "  - 선형 회귀에서는 데이터와 레이블 사이에 선형 관계가 있다고 가정하며, 따라서 일반적으로 y = Wx + b와 같은 형태의 가설을 수립함.\n",
        "  - 이때, 데이터에 곱해지는 W = 가중치(Weight)\n",
        "  - 더해지는 값 = 편향(bias)라고 함.\n",
        "  - Linear Regression의 목표: 주어진 학습 데이터에 가장 알맞은 W,b를 찾아내는 것.(주어진 데이터 셋을 가장 잘 설명하는 선을 그리는 일)\n",
        "  - 주어진 학습 데이터에 가장 알맞는다는 것은 어떤 기준을 세우느냐에 따라 달라질 수 있음.\n",
        "    - ex) 각 데이터에서의 오차 평균값을 최소화하려는 상황, 각 데이터 포인트에서의 오차에 대한 중요도가 조금씩 다른 상황 등\n",
        "  - 이에 학습 전에 머신러닝 모델이 현재 주어진 데이터 셋에 얼마나 알맞은 상태인지 측정할 수 있는 기준을 세워야 하며 이를 아래와 같은 용어로 부른다.\n",
        "    - 비용 함수(cost function)\n",
        "    - 손실 함수(loss function)\n",
        "    - 오차 함수(error function)\n",
        "    - 목적 함수(objective function)\n",
        "  - 직선이 주어진 데이터 셋에 적합한 정도를 수치화하기 위하여 오차 개념 도입\n",
        "    - 오차 = 실제값(레이블) - 예측값(prediction)\n",
        "  - 오차는 음수로 나올 수 있지만, 중요한 것은 절대값의 크기라고 할 수 있음.\n",
        "  - 정의한 오차를 제곱하여 새로운 오차 개념을 정의함.\n",
        "  - (실제값 - 예측값)^2을 모든 데이터에 대해서 구해 합한다.\n",
        "  - 총 데이터 개수가 n개이므로 평균값을 구함.\n",
        "    - 평균 제곱 오차(Mean Squared Error, MSE)라고 하며 널리 쓰이고 간단한 형태 함수 중 하나임.\n",
        "  - MSE를 손실 함수로 정했다면, 우리가 원하는 선형 회귀 목표는 MSE값을 최소로 하는 W, b의 값을 찾는 것이 됨.\n",
        "  - 우리가 원하는 이상적인 W, b값으 찾기 위해서 현대 머신러닝에서는 경사하강법(gradient descent)라는 최적화 기법을 주로 활용함.\n",
        "    - 선형 회귀의 경우, 경사하강법 대신 행렬 연산으로 바로 최적값을 구할 수 있으나, 더욱 복잡한 머신러닝 모델들에 대해서는 경사하강법이 필수라고 할 수 있음.\n",
        "  - 경사하강법(gradient descent)\n",
        "    - 변경하는 방향은 미분을 통해서 알 수 있으며, 학습률(learning rate)라는 별도의 값을 통해서 한번에 변경하는 값의 양을 결정함.  \n"
      ],
      "metadata": {
        "id": "j7xg2RZrLFbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "'''\n",
        "torch.manual_seed(): PyTorch에서 난수 생성기를 초기화하는 함수임.\n",
        "torch.manual_seed(1): 난수 생성기의 상태가 고정되어 동일한 난수가 생성되도록 함.\n",
        "PyTorch에서는 모델 학습에서 가중치 초기화, 데이터 샘플링, 배치 구성 등 여러 작업에\n",
        "난수가 사용되는데, torch.manual_seed()를 사용하여 난수의 생성 순서를 고정하면\n",
        "코드 실행 시마다 동일한 결과를 얻을 수 있습니다.\n",
        "\n",
        "왜 이걸 쓰는가?\n",
        "1. 재현성 보장: 같은 난수 사용 -> 동일한 결과를 얻고자 할 때 유용함.\n",
        "2. 디버깅: 문제를 재현하여 쉽게 디버깅할 수 있도록 도와줌.\n",
        "\n",
        "torch.manual_seed(1)은 코드 실행 시 매번 동일한 난수 시퀀스를 제공하여\n",
        "모델 훈련이나 실험의 재현성을 보장하기 위해 사용됨.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "QuDt1fL97_6n",
        "outputId": "5ec3b241-e03f-400c-f9c7-8c50cb472e6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorch.manual_seed(): PyTorch에서 난수 생성기를 초기화하는 함수임.\\ntorch.manual_seed(1): 난수 생성기의 상태가 고정되어 동일한 난수가 생성되도록 함. \\nPyTorch에서는 모델 학습에서 가중치 초기화, 데이터 샘플링, 배치 구성 등 여러 작업에 \\n난수가 사용되는데, torch.manual_seed()를 사용하여 난수의 생성 순서를 고정하면 \\n코드 실행 시마다 동일한 결과를 얻을 수 있습니다.\\n\\n왜 이걸 쓰는가?\\n1. 재현성 보장: 같은 난수 사용 -> 동일한 결과를 얻고자 할 때 유용함.\\n2. 디버깅: 문제를 재현하여 쉽게 디버깅할 수 있도록 도와줌.\\n\\ntorch.manual_seed(1)은 코드 실행 시 매번 동일한 난수 시퀀스를 제공하여 \\n모델 훈련이나 실험의 재현성을 보장하기 위해 사용됨.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋 생성\n",
        "# 보통 PyTorch에서는 데이터 셋을 나타내는 텐서의 경우, 첫번째 차원을 데이터의 순번을 나타내기 위한 차원으로 사용함.\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[3],[6],[9]])"
      ],
      "metadata": {
        "id": "RX2OafeA8d6e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "print(x_train.shape)\n",
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mql02ia8v7q",
        "outputId": "66ac8490-66ec-4373-f36f-7c1ff3ba5e2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n",
            "tensor([[3.],\n",
            "        [6.],\n",
            "        [9.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 및 편향 생성\n",
        "# requires_grad=True 옵션을 통해 autograd를 이용하기 위한 변수임을 나타낼 수 있음.\n",
        "# False로 입력하게 되면, autograd를 사용할 수 없으며 손수 미분값을 계산 및 업데이트 해야함.\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(W)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEilECfbAs_Y",
        "outputId": "90ec9f9a-6427-4c49-9e70-d1fe326b08ed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n",
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가설 수립\n",
        "# W, b를 이용한 전체 가설을 수립함.\n",
        "# PyTorch에서는 W*x+b와 같이 실제 가설의 수학식을 가설로 선언\n",
        "hypo = x_train * W + b\n",
        "print(hypo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLjfBUSQE1ju",
        "outputId": "03daf804-fe72-48c6-a7c1-a1acbb06d848"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 혹은 비용 함수를 선언하여 학습 준비\n",
        "# 비용 함수 선언\n",
        "cost = torch.mean((hypo - y_train)**2)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rTReri0FSlf",
        "outputId": "fddceebb-9f6f-45cf-dea4-7fdb54c48672"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(42., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 경사 하강법 구현\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W,b],lr=0.01)\n",
        "nb_epochs = 1000 # 1000번 경사하강법 반복"
      ],
      "metadata": {
        "id": "FbCu5PozFiMz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주어진 손실 함수에 대한 미분값을 구하는 과정은 .backward()로 이루어지며, step()을 통해서 W, b값이 조금씩 바뀌게 됨.\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  # 가설 및 손실 함수\n",
        "  hypo = x_train * W + b\n",
        "  cost = torch.mean((hypo - y_train)**2)\n",
        "  # 경사하강법\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  # 100번 마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch}/{nb_epochs} W: {W.item():.3f}, b: {b.item():.3f}, Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw55lhoaG41D",
        "outputId": "37f1367b-e7ba-411b-e739-4e8867c0dcda"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 W: 0.280, b: 0.120, Cost: 42.000000\n",
            "Epoch 100/1000 W: 2.619, b: 0.867, Cost: 0.108384\n",
            "Epoch 200/1000 W: 2.700, b: 0.682, Cost: 0.066975\n",
            "Epoch 300/1000 W: 2.764, b: 0.536, Cost: 0.041386\n",
            "Epoch 400/1000 W: 2.815, b: 0.421, Cost: 0.025574\n",
            "Epoch 500/1000 W: 2.854, b: 0.331, Cost: 0.015803\n",
            "Epoch 600/1000 W: 2.886, b: 0.260, Cost: 0.009765\n",
            "Epoch 700/1000 W: 2.910, b: 0.205, Cost: 0.006034\n",
            "Epoch 800/1000 W: 2.929, b: 0.161, Cost: 0.003729\n",
            "Epoch 900/1000 W: 2.944, b: 0.126, Cost: 0.002304\n",
            "Epoch 1000/1000 W: 2.956, b: 0.099, Cost: 0.001424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 선형 회귀 학습 코드 구현\n",
        "- 학습하는 동안에는 매 에포크마다 아래의 과정이 진행된다.\n",
        "  - 1. 현재 weight 및 bias를 이용한 예측값 도출\n",
        "  - 2. 예측값과 레이블을 이용해서 손실값 도출\n",
        "  - 3. 미분값 초기화\n",
        "  - 4. 미분값 계산\n",
        "  - 5. W, b 값 업데이트\n",
        "\n",
        "- 미분값을 초기화하지 않고 계산하면, 계속해서 미분값들이 쌓이게 되어 있으므로 업데이트에 사용한 후에 미분값을 초기화해주어야 함."
      ],
      "metadata": {
        "id": "X2BG2mQ6IRVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 선형 회귀 추론 코드 구현\n",
        "\n",
        "# 아직 결과값을 모르는 데이터에 대한 추론\n",
        "test_data = torch.FloatTensor([[4.]])\n",
        "prediction = test_data * W + b\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpRn44SVJpDP",
        "outputId": "61460a09-c568-4770-dcbd-becf04ae98f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11.9245]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}
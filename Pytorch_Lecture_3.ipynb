{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression : Linear regression is finding a straight line that best fits the training data. And it is W and b that define the best-fitting straight line. The goal of linear regression is to find the values ​​of W and b that define the best-fitting straight line."
      ],
      "metadata": {
        "id": "wRpt_p9cnl7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Example"
      ],
      "metadata": {
        "id": "1jqtjMqXrrlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1) # Set the random seed so that the random value is the same even if you rerun the Python code."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAii5QP7nlrV",
        "outputId": "e2d3fb8b-3bf7-4b79-a941-ff10cb93afa1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ad6586a5410>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dataset.\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[3],[5],[7]])\n",
        "print(x_train)\n",
        "print(x_train.shape,'\\n')\n",
        "print(y_train)\n",
        "print(y_train.shape,'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toc9gMHEoTQ6",
        "outputId": "9285da68-1e55-4fc8-dbcf-c74d0c88d2d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1]) \n",
            "\n",
            "tensor([[3.],\n",
            "        [5.],\n",
            "        [7.]])\n",
            "torch.Size([3, 1]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set weight, bais.\n",
        "W = torch.zeros(1, requires_grad=True) # if \"requires = True\", it means that this variable is a variable whose value continues to change through learning.\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(W,'\\n')\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O9jf9ACozP7",
        "outputId": "bfa122aa-0204-4fde-a157-9a9a01174237"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True) \n",
            "\n",
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Formulate a hypothesis.\n",
        "hypo = x_train * W + b\n",
        "print(hypo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTTPYLOqVjv",
        "outputId": "38d0c44e-a580-410e-8147-67cb2678c28b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for learning by declaring the cost function.\n",
        "cost = torch.mean((hypo - y_train)**2)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWq7rA8FqvaX",
        "outputId": "eab48c6a-086c-4c76-8e80-05c0c8665563"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(27.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression learning code implementation"
      ],
      "metadata": {
        "id": "4ytGzbmgr1vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent method implementation and learning progress Full code implementation\n",
        "\n",
        "# Set dataset\n",
        "x_train = torch.FloatTensor([1,2,3]).view(-1,1)\n",
        "y_train = torch.FloatTensor([5,10,15]).view(-1,1)\n",
        "\n",
        "# Model initialization\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = optim.SGD([W,b], lr=0.01)\n",
        "\n",
        "# Repeat gradient descent as many times as desired\n",
        "nb_epochs = 10000"
      ],
      "metadata": {
        "id": "ozGCyMc9rKPH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During learning, the following process occurs every epoch.\n",
        "- 1. Derive forecasts using current weights and biases\n",
        "- 2. Derive loss value using predicted value and label\n",
        "- 3. Differential initialization\n",
        "- 4. Differential Calculation\n",
        "- 5. W, b values update\n"
      ],
      "metadata": {
        "id": "KX-FtCsyx6hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch : Iterative unit that learns the entire training dataset once\n",
        "for epoch in range(nb_epochs+1):\n",
        "  # hypothesis, cost function progress\n",
        "  hypo = x_train * W + b\n",
        "  cost = torch.mean((hypo - y_train)**2)\n",
        "\n",
        "  # gradient descent progress\n",
        "  optimizer.zero_grad() # In Pytorch, gradient values ​​are continuously added when doing backwards, so we should always start by setting the gradients to zero before doing backpropagation.\n",
        "  cost.backward() # Finding the gradient by differentiating the cost function during the learning process is called a backward operation. cost.backward() means finding the slope from the cost function and is a backward operation.\n",
        "  optimizer.step() # Update parameters using the gradient values ​​stored as the parameters of each layer. This command updates parameters and improves model performance.\n",
        "\n",
        "  # Log output every 100 times\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch:4d}/{nb_epochs} W:{W.item():.3f}, b:{b.item():.3f} Cost:{cost.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsbcuosStgkK",
        "outputId": "878ed71c-4bfb-4edd-abba-bccb89c1eacf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/10000 W:0.467, b:0.200 Cost:116.6667\n",
            "Epoch  100/10000 W:4.364, b:1.445 Cost:0.3011\n",
            "Epoch  200/10000 W:4.500, b:1.136 Cost:0.1860\n",
            "Epoch  300/10000 W:4.607, b:0.893 Cost:0.1150\n",
            "Epoch  400/10000 W:4.691, b:0.702 Cost:0.0710\n",
            "Epoch  500/10000 W:4.757, b:0.552 Cost:0.0439\n",
            "Epoch  600/10000 W:4.809, b:0.434 Cost:0.0271\n",
            "Epoch  700/10000 W:4.850, b:0.341 Cost:0.0168\n",
            "Epoch  800/10000 W:4.882, b:0.268 Cost:0.0104\n",
            "Epoch  900/10000 W:4.907, b:0.211 Cost:0.0064\n",
            "Epoch 1000/10000 W:4.927, b:0.166 Cost:0.0040\n",
            "Epoch 1100/10000 W:4.943, b:0.130 Cost:0.0024\n",
            "Epoch 1200/10000 W:4.955, b:0.102 Cost:0.0015\n",
            "Epoch 1300/10000 W:4.965, b:0.080 Cost:0.0009\n",
            "Epoch 1400/10000 W:4.972, b:0.063 Cost:0.0006\n",
            "Epoch 1500/10000 W:4.978, b:0.050 Cost:0.0004\n",
            "Epoch 1600/10000 W:4.983, b:0.039 Cost:0.0002\n",
            "Epoch 1700/10000 W:4.986, b:0.031 Cost:0.0001\n",
            "Epoch 1800/10000 W:4.989, b:0.024 Cost:0.0001\n",
            "Epoch 1900/10000 W:4.992, b:0.019 Cost:0.0001\n",
            "Epoch 2000/10000 W:4.993, b:0.015 Cost:0.0000\n",
            "Epoch 2100/10000 W:4.995, b:0.012 Cost:0.0000\n",
            "Epoch 2200/10000 W:4.996, b:0.009 Cost:0.0000\n",
            "Epoch 2300/10000 W:4.997, b:0.007 Cost:0.0000\n",
            "Epoch 2400/10000 W:4.997, b:0.006 Cost:0.0000\n",
            "Epoch 2500/10000 W:4.998, b:0.004 Cost:0.0000\n",
            "Epoch 2600/10000 W:4.998, b:0.004 Cost:0.0000\n",
            "Epoch 2700/10000 W:4.999, b:0.003 Cost:0.0000\n",
            "Epoch 2800/10000 W:4.999, b:0.002 Cost:0.0000\n",
            "Epoch 2900/10000 W:4.999, b:0.002 Cost:0.0000\n",
            "Epoch 3000/10000 W:4.999, b:0.001 Cost:0.0000\n",
            "Epoch 3100/10000 W:5.000, b:0.001 Cost:0.0000\n",
            "Epoch 3200/10000 W:5.000, b:0.001 Cost:0.0000\n",
            "Epoch 3300/10000 W:5.000, b:0.001 Cost:0.0000\n",
            "Epoch 3400/10000 W:5.000, b:0.001 Cost:0.0000\n",
            "Epoch 3500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 3600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 3700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 3800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 3900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 4900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 5900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 6900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 7900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 8900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9000/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9100/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9200/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9300/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9400/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9500/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9600/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9700/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9800/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 9900/10000 W:5.000, b:0.000 Cost:0.0000\n",
            "Epoch 10000/10000 W:5.000, b:0.000 Cost:0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression inference code implementation\n"
      ],
      "metadata": {
        "id": "BZY-DeDxy4Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference about data whose outcome is unknown\n",
        "test_data = torch.FloatTensor([[5]])\n",
        "prediction = test_data * W + b\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6tVMGVUw6kY",
        "outputId": "9fb074d4-4b17-4c5f-fd7e-088a24dff0ac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24.9999]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}